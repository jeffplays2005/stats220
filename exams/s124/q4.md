## Q4.1

Would it be appropriate for the student to scrape data from this website?
You should access the website and describe what you checked as part of your answer.

```
Upon checking the `robots.txt` file on the site, there isn't explicit mentions of not allowing scraping, although upon checking the T&Cs, it explicitly states under part 1, clause 14 "No interference" to no scraping. Based on this, it's clear it would not be appropriate for the student to scrape data from this website.
```

Example:

```
First, I went on to the website and found the T&C page at the bottom. On this page, I used ctrl+f and searched for "scrape," and I found under point 14 of the general website terms and conditions section that by using the website, I, "agree not to do anything that may cause HOTC undue inconvenience, disruption or offence, or that may affect the security or operation of our websites, any services offered via our websites or any network or system underlying or connected to them (including without limitation, by using a robot, spider, scraper or other automated means to access this website or feature on it for any purpose)." Based on this, it's clear it would not be appropriate for the student to scrape data from this website.
```

## Q4.2

The student decided to focus on local tourism instead, and scraped data from the web
page https://www.hobbitontours.com/experiences/.

Their first goal was to create a vector of URLs for the individual pages of the 14 experiences
listed on the web page.

The student used the Inspect tool in their browser to identify the relevant HTML element, as
shown below:

![](https://media.discordapp.net/attachments/1294601014628061238/1386272994896253041/Screenshot_2025-06-22_at_9.13.52_PM.png?ex=68591ad5&is=6857c955&hm=21d8866d76e07c4df770dd9da0d1af00dc57c62b82d35e57ccd31c73a2952baf&=&quality=lossless)

The R code they then wrote produced the following output:

```
[1] "/experiences/hobbiton-movie-set-tour/" "/experiences/matamata-isite/"
[3] "/experiences/mandarin/" "/experiences/tourmealcombo/"
[5] "/experiences/evening-banquet-tour/" "/experiences/second-breakfast/"
[7] “/experiences/private-tour/" "/experiences/gift-voucher/"
[9] "/experiences/summer-harvest/" "/halflingmarathon/"
[11] "/experiences/mid-winter-feast/" "/experiences/hobbitday/"
[13] "/experiences/beer-fest/" "/experiences/hobbiton-christmas/"
```

The code below provides the code the student wrote to create the output above, but some parts
of the code have been replaced with numbers e.g. {1}

```R
library(tidyverse)
library({1})
url <- "https://www.hobbitontours.com/experiences/"

experience_urls <- {2}({3}) %>%
  html_{4}("{5}c-pathway__title") %>%
  html_{6}("{7}")

{8}
```

- {1} - `rvest`
- {2} - `read_html`
- {3} - `url`
- {4} `elements`
- {5} `.`
- {6} - `attr`
- {7} - `href`
- {8} - `experience_urls`

## Q4.3

The same student started to write more R code to scrape data from each of the 14 pages using
the URLs in the vector `experience_urls` (from Q4.2) but got an error. They know they need to write some additional code (see below).

```R
experience_urls_fixed <- experience_urls %>%
  # Write additional code here
```

Write the additional R code that is needed to “fix” the URLs so they can be used for web scraping using the function `read_html()`.

```R
experience_urls_fixed <- experience_urls %>%
  paste0("https://www.hobbitontours.com", .)
```

## Q4.4

For Project 5, a student decided to focus on the use of the words “tourism” and “employment” in
the speeches published on the beehive.govt website.
An early version of the plot they produced using their data frame `speech_data` is shown below.

![](https://cdn.discordapp.com/attachments/760750613431582773/1386282597210128435/image.png?ex=685923c7&is=6857d247&hm=8f7c92589e4ec0077aa8a9eaf15a87e3bc40cb849be7a3fb8c53442813d11ed9&)

The code the student used to produce the plot show above is given below.

```R
speech_data %>%
  ggplot() +
  geom_line(aes(x = year,
    y = per_mentioned,
    group = keyword,
    linetype = keyword)) +
  geom_point(aes(x = year,
    y = per_mentioned,
    color = keyword)) +
  theme_minimal()
```

The final plot they submitted for their project is shown below.

![](https://cdn.discordapp.com/attachments/760750613431582773/1386284006848466944/image.png?ex=68592517&is=6857d397&hm=5ba6eb5c2305432e1243df4cacbd0c02c3e2a6b9068969430be15a173ab5a072&)

Describe six additional ggplot layers needed to transform the first plot into the second plot.

You do not need to write code for each layer, but you do need to refer to the relevant function name and provide enough detail to make it clear you know why this layer is needed to produce the second plot.

1. The use of labs to label the `x` and `y` axis from the default `per_mentioned` and `year` to the correct descriptive values such as "Proportion of speecbes that included this word" and "Year", also a caption for the data source of `beehive.govt.nz`.
2. Need to use `annotate` to label the two lines.
3. Need to change the color of `tourism` and `employment` using `scale_fill_manual`
4. Use `guides()` to remove the legend
5. Remove the plot background using theme layer
6. Manually change the scales to 0.1-0.5 (y-axis) and 1995-2023 (x-axis) using `scale_x_continuous(limits=c(1995, 2023))` and `scale_y_continuous(limits=c(0.1, 0.5))`

## Q4.5

Recall from lectures that we explored data about the number of free (available) car parks in the
Civic car park building.

Describe one question that you could answer by exploring this data. Then, in no more than two
sentences, briefly describe the data manipulations required to answer your question.

The question you describe can not be one explored in the lectures.

```
What days of the week does the Civic car park have the most free spaces?
```

Briefly describe the data manipulations required to answer your question, in no more
than two sentences.

We can `group_by` the data based on the week day, and `summarise` the free spots by using `mean()`

Mock answer:

```
We would need to use filter() function to filter for only the location of Civic car park and make sure we have the day of week using mutate() and wday(). Then we would use group_by() to group the weeks and find the mean of each of them using mean().
```

Mock Set 2:

```
What is the peak time for the civic car park building depending on the minimum number of free spaces during
January 1st 2025?

First, I would group by civic and then I would reorder free-spaces by minimum to maximum (i.e. ascending)
and then take the first row of data and then you can convert the timestamp and find the hour.
```
